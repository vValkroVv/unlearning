# @package _global_

defaults:
  - override /model: Llama-3.1-8B-Instruct-lora
  - override /trainer: UNDIAL
  - override /data: unlearn
  - override /data/datasets@data.forget: DUET_QA_forget
  - override /data/datasets@data.retain: DUET_QA_retain
  - override /eval: duet

forget_split: city_forget_popular_5
retain_split: city_fast_retain_500
holdout_split: ${retain_split}
retain_logs_path: null
question_key: question

model:
  model_args:
    pretrained_model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
    # Override with +model.model_args.pretrained_model_name_or_path=/path/to/local/checkpoint to use SFT weights

data:
  anchor: forget

eval:
  duet:
    forget_split: ${forget_split}
    holdout_split: ${holdout_split}
    retain_logs_path: ${retain_logs_path}
    question_key: ${question_key}

trainer:
  args:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 32
    learning_rate: 1e-4
    num_train_epochs: 5 #5
    lr_scheduler_type: constant
    warmup_ratio: 0.1
    logging_steps: 10
    eval_strategy: "no"
    save_strategy: "no"
    do_eval: false
    eval_on_start: false
    remove_unused_columns: false
    gradient_checkpointing: true
    ddp_find_unused_parameters: false
  method_args:
    beta: 3
    gamma: 1.0
    alpha: 0.1
    retain_loss_type: NLL

task_name: duet_undial_lora_popular
